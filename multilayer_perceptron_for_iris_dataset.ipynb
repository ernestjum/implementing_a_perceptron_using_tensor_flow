{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f0761de",
   "metadata": {},
   "source": [
    "# Builtding a Multilayer Perceptron for Classifying Flowers in Iris Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c635744",
   "metadata": {},
   "source": [
    "We use tf.keras.layers which provide readily avialable building blocks of a Nueral Network (NN) model. We will use these layers to solve a classification problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b1ef8ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "88571035",
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_fn():\n",
    "    split = tfds.Split.TRAIN\n",
    "    dataset = tfds.load('iris', split=split, as_supervised=True)\n",
    "    dataset = dataset.map(lambda features, labels: ({'dense_input':features}, labels))\n",
    "    dataset = dataset.batch(50).repeat()\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "818c5476",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dense_input': <tf.Tensor: shape=(50, 4), dtype=float32, numpy=\n",
      "array([[5.1, 3.4, 1.5, 0.2],\n",
      "       [7.7, 3. , 6.1, 2.3],\n",
      "       [5.7, 2.8, 4.5, 1.3],\n",
      "       [6.8, 3.2, 5.9, 2.3],\n",
      "       [5.2, 3.4, 1.4, 0.2],\n",
      "       [5.6, 2.9, 3.6, 1.3],\n",
      "       [5.5, 2.6, 4.4, 1.2],\n",
      "       [5.5, 2.4, 3.7, 1. ],\n",
      "       [4.6, 3.4, 1.4, 0.3],\n",
      "       [7.7, 2.8, 6.7, 2. ],\n",
      "       [7. , 3.2, 4.7, 1.4],\n",
      "       [4.6, 3.2, 1.4, 0.2],\n",
      "       [6.5, 3. , 5.2, 2. ],\n",
      "       [5.5, 4.2, 1.4, 0.2],\n",
      "       [5.4, 3.9, 1.3, 0.4],\n",
      "       [5. , 3.5, 1.3, 0.3],\n",
      "       [5.1, 3.8, 1.5, 0.3],\n",
      "       [4.8, 3. , 1.4, 0.1],\n",
      "       [6.5, 3. , 5.8, 2.2],\n",
      "       [7.6, 3. , 6.6, 2.1],\n",
      "       [6.7, 3.3, 5.7, 2.1],\n",
      "       [7.9, 3.8, 6.4, 2. ],\n",
      "       [6.7, 3. , 5.2, 2.3],\n",
      "       [5.8, 4. , 1.2, 0.2],\n",
      "       [6.3, 2.5, 5. , 1.9],\n",
      "       [5. , 3. , 1.6, 0.2],\n",
      "       [6.9, 3.1, 5.1, 2.3],\n",
      "       [6.1, 3. , 4.6, 1.4],\n",
      "       [5.8, 2.7, 4.1, 1. ],\n",
      "       [5.2, 2.7, 3.9, 1.4],\n",
      "       [6.7, 3. , 5. , 1.7],\n",
      "       [5.7, 2.6, 3.5, 1. ],\n",
      "       [5.8, 2.8, 5.1, 2.4],\n",
      "       [6.5, 3.2, 5.1, 2. ],\n",
      "       [5.7, 3.8, 1.7, 0.3],\n",
      "       [5.4, 3.9, 1.7, 0.4],\n",
      "       [5.2, 3.5, 1.5, 0.2],\n",
      "       [4.9, 3.1, 1.5, 0.1],\n",
      "       [5.6, 2.7, 4.2, 1.3],\n",
      "       [5. , 3.4, 1.6, 0.4],\n",
      "       [5.3, 3.7, 1.5, 0.2],\n",
      "       [6.6, 2.9, 4.6, 1.3],\n",
      "       [5.5, 3.5, 1.3, 0.2],\n",
      "       [6.7, 3.1, 4.4, 1.4],\n",
      "       [4.3, 3. , 1.1, 0.1],\n",
      "       [6.4, 3.1, 5.5, 1.8],\n",
      "       [5. , 3.2, 1.2, 0.2],\n",
      "       [5.7, 4.4, 1.5, 0.4],\n",
      "       [6. , 2.2, 4. , 1. ],\n",
      "       [6.1, 2.6, 5.6, 1.4]], dtype=float32)>}\n",
      "tf.Tensor(\n",
      "[0 2 1 2 0 1 1 1 0 2 1 0 2 0 0 0 0 0 2 2 2 2 2 0 2 0 2 1 1 1 1 1 2 2 0 0 0\n",
      " 0 1 0 0 1 0 1 0 2 0 0 1 2], shape=(50,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "for features_batch, labels_batch in input_fn().take(1):\n",
    "    print(features_batch)\n",
    "    print(labels_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "453f2dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "iris, iris_info = tfds.load('iris', with_info=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1f68966c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tfds.core.DatasetInfo(\n",
      "    name='iris',\n",
      "    full_name='iris/2.1.0',\n",
      "    description=\"\"\"\n",
      "    This is perhaps the best known database to be found in the pattern recognition\n",
      "    literature. Fisher's paper is a classic in the field and is referenced\n",
      "    frequently to this day. (See Duda & Hart, for example.) The data set contains\n",
      "    3 classes of 50 instances each, where each class refers to a type of iris\n",
      "    plant. One class is linearly separable from the other 2; the latter are NOT\n",
      "    linearly separable from each other.\n",
      "    \"\"\",\n",
      "    homepage='https://archive.ics.uci.edu/ml/datasets/iris',\n",
      "    data_dir='C:\\\\Users\\\\jumer\\\\tensorflow_datasets\\\\iris\\\\2.1.0',\n",
      "    file_format=tfrecord,\n",
      "    download_size=3.65 KiB,\n",
      "    dataset_size=7.62 KiB,\n",
      "    features=FeaturesDict({\n",
      "        'features': Tensor(shape=(4,), dtype=float32),\n",
      "        'label': ClassLabel(shape=(), dtype=int64, num_classes=3),\n",
      "    }),\n",
      "    supervised_keys=('features', 'label'),\n",
      "    disable_shuffling=False,\n",
      "    splits={\n",
      "        'train': <SplitInfo num_examples=150, num_shards=1>,\n",
      "    },\n",
      "    citation=\"\"\"@misc{Dua:2019 ,\n",
      "    author = \"Dua, Dheeru and Graff, Casey\",\n",
      "    year = \"2017\",\n",
      "    title = \"{UCI} Machine Learning Repository\",\n",
      "    url = \"http://archive.ics.uci.edu/ml\",\n",
      "    institution = \"University of California, Irvine, School of Information and Computer Sciences\"\n",
      "    }\"\"\",\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(iris_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "975fe85a",
   "metadata": {},
   "source": [
    "This dataset comes with just one partition as seen in the information printed above, i.e., splits = {'train':....}. As such, we will need to  split the dataset into training, validation, and testing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ebb64a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(1)\n",
    "original_ds = iris['train']\n",
    "original_da = original_ds.shuffle(150, reshuffle_each_iteration=False)\n",
    "\n",
    "original_ds_train = original_ds.take(100)\n",
    "original_ds_test = original_ds.skip(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "407f8e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_ds_train = original_ds_train.map(\n",
    "    lambda x : (x['features'], x['label']))\n",
    "\n",
    "original_ds_test = original_ds_test.map(\n",
    "    lambda x : (x['features'], x['label']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "79cbbe0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(16, activation='sigmoid',\n",
    "                   name='fc1', input_shape=(4, )),\n",
    "    tf.keras.layers.Dense(3, activation='softmax',\n",
    "                 name='fc2')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9cd247ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " fc1 (Dense)                 (None, 16)                80        \n",
      "                                                                 \n",
      " fc2 (Dense)                 (None, 3)                 51        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 131 (524.00 Byte)\n",
      "Trainable params: 131 (524.00 Byte)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "iris_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "edd65614",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "iris_model.compile(optimizer='adam',\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b35554b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 100\n",
    "training_size = 100\n",
    "batch_size = 2\n",
    "steps_per_epoch = np.ceil(training_size/batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7ba2382c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = original_ds_train.shuffle(buffer_size=training_size)\n",
    "train_ds = train_ds.repeat()\n",
    "train_ds = train_ds.batch(batch_size=batch_size)\n",
    "train_ds = train_ds.prefetch(buffer_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "413be340",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"C:\\Users\\jumer\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1401, in train_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\jumer\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1384, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\jumer\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1373, in run_step  **\n        outputs = model.train_step(data)\n    File \"C:\\Users\\jumer\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1151, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"C:\\Users\\jumer\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1209, in compute_loss\n        return self.compiled_loss(\n    File \"C:\\Users\\jumer\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\compile_utils.py\", line 277, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"C:\\Users\\jumer\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py\", line 143, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"C:\\Users\\jumer\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py\", line 270, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"C:\\Users\\jumer\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py\", line 2221, in categorical_crossentropy\n        return backend.categorical_crossentropy(\n    File \"C:\\Users\\jumer\\anaconda3\\Lib\\site-packages\\keras\\src\\backend.py\", line 5573, in categorical_crossentropy\n        target.shape.assert_is_compatible_with(output.shape)\n\n    ValueError: Shapes (None, 1) and (None, 3) are incompatible\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[38], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m history \u001b[38;5;241m=\u001b[39m iris_model\u001b[38;5;241m.\u001b[39mfit(train_ds, epochs\u001b[38;5;241m=\u001b[39mnum_epochs,\n\u001b[0;32m      2\u001b[0m                         steps_per_epoch\u001b[38;5;241m=\u001b[39msteps_per_epoch,\n\u001b[0;32m      3\u001b[0m                         verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_filejb9zdbqf.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"C:\\Users\\jumer\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1401, in train_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\jumer\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1384, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\jumer\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1373, in run_step  **\n        outputs = model.train_step(data)\n    File \"C:\\Users\\jumer\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1151, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"C:\\Users\\jumer\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1209, in compute_loss\n        return self.compiled_loss(\n    File \"C:\\Users\\jumer\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\compile_utils.py\", line 277, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"C:\\Users\\jumer\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py\", line 143, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"C:\\Users\\jumer\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py\", line 270, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"C:\\Users\\jumer\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py\", line 2221, in categorical_crossentropy\n        return backend.categorical_crossentropy(\n    File \"C:\\Users\\jumer\\anaconda3\\Lib\\site-packages\\keras\\src\\backend.py\", line 5573, in categorical_crossentropy\n        target.shape.assert_is_compatible_with(output.shape)\n\n    ValueError: Shapes (None, 1) and (None, 3) are incompatible\n"
     ]
    }
   ],
   "source": [
    "history = iris_model.fit(train_ds, epochs=num_epochs,\n",
    "                        steps_per_epoch=steps_per_epoch,\n",
    "                        verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bee763f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
